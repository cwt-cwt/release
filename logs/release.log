17:32:14.227 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:32:16.077 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:32:16.247 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 曹曹,²ܲ
17:32:16.249 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 曹曹,²ܲ
17:32:16.283 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:32:16.284 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:32:16.301 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(曹曹, ²ܲ); groups with view permissions: Set(); users  with modify permissions: Set(曹曹, ²ܲ); groups with modify permissions: Set()
17:32:18.450 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5591.
17:32:18.558 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:32:18.667 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:32:18.688 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:32:18.688 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:32:18.708 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\曹曹\AppData\Local\Temp\blockmgr-1f527b5e-ffb9-4c5f-9d07-e4a7d150449c
17:32:18.862 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1994.1 MB
17:32:19.042 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:32:19.501 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @20957ms
17:32:19.857 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:32:19.965 INFO  [main] org.spark_project.jetty.server.Server - Started @21423ms
17:32:20.029 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@3f3c966c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:32:20.030 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:32:20.099 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c4ee95c{/jobs,null,AVAILABLE,@Spark}
17:32:20.100 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a1d3c1a{/jobs/json,null,AVAILABLE,@Spark}
17:32:20.101 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@159e366{/jobs/job,null,AVAILABLE,@Spark}
17:32:20.103 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17ae98d7{/jobs/job/json,null,AVAILABLE,@Spark}
17:32:20.104 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac4944a{/stages,null,AVAILABLE,@Spark}
17:32:20.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39fc6b2c{/stages/json,null,AVAILABLE,@Spark}
17:32:20.106 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3ee39da0{/stages/stage,null,AVAILABLE,@Spark}
17:32:20.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c0b41d6{/stages/stage/json,null,AVAILABLE,@Spark}
17:32:20.110 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bffa76d{/stages/pool,null,AVAILABLE,@Spark}
17:32:20.111 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/stages/pool/json,null,AVAILABLE,@Spark}
17:32:20.113 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/storage,null,AVAILABLE,@Spark}
17:32:20.114 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/storage/json,null,AVAILABLE,@Spark}
17:32:20.115 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/storage/rdd,null,AVAILABLE,@Spark}
17:32:20.117 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/storage/rdd/json,null,AVAILABLE,@Spark}
17:32:20.118 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/environment,null,AVAILABLE,@Spark}
17:32:20.119 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5215cd9a{/environment/json,null,AVAILABLE,@Spark}
17:32:20.121 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/executors,null,AVAILABLE,@Spark}
17:32:20.127 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/executors/json,null,AVAILABLE,@Spark}
17:32:20.128 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/executors/threadDump,null,AVAILABLE,@Spark}
17:32:20.129 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:32:20.142 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3d9fc57a{/static,null,AVAILABLE,@Spark}
17:32:20.145 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6650813a{/,null,AVAILABLE,@Spark}
17:32:20.147 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50cf5a23{/api,null,AVAILABLE,@Spark}
17:32:20.149 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20765ed5{/jobs/job/kill,null,AVAILABLE,@Spark}
17:32:20.150 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2899a8db{/stages/stage/kill,null,AVAILABLE,@Spark}
17:32:20.161 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.182.1:4040
17:32:20.617 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:32:20.732 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5612.
17:32:20.743 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.182.1:5612
17:32:20.757 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:32:20.888 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.182.1, 5612, None)
17:32:20.923 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.182.1:5612 with 1994.1 MB RAM, BlockManagerId(driver, 192.168.182.1, 5612, None)
17:32:20.939 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.182.1, 5612, None)
17:32:20.940 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.182.1, 5612, None)
17:32:21.369 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@614aeccc{/metrics/json,null,AVAILABLE,@Spark}
17:32:27.053 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/release/target/classes/hive-site.xml
17:32:27.113 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('hdfs://hdfsCluster/hive/db').
17:32:27.115 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'hdfs://hdfsCluster/hive/db'.
17:32:27.134 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f4596d0{/SQL,null,AVAILABLE,@Spark}
17:32:27.135 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@588545ac{/SQL/json,null,AVAILABLE,@Spark}
17:32:27.136 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@159424e2{/SQL/execution,null,AVAILABLE,@Spark}
17:32:27.137 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e54a6b1{/SQL/execution/json,null,AVAILABLE,@Spark}
17:32:27.141 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@469a7575{/static/sql,null,AVAILABLE,@Spark}
17:32:28.751 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:32:29.778 WARN  [main] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.metastore.local does not exist
17:32:30.352 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://Host01:9083
17:32:30.542 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for ²ܲ: Unknown error.
17:32:30.993 INFO  [main] hive.metastore - Connected to metastore.
17:32:45.612 WARN  [main] org.apache.hadoop.hdfs.DFSUtil - Namenode for hdfsCluster remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.
17:32:47.878 WARN  [main] org.apache.hadoop.hdfs.DFSUtil - Namenode for hdfsCluster remains unresolved for ID nn2.  Check your hdfs-site.xml file to ensure namenodes are configured properly.
17:32:47.984 ERROR [main] com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$ - Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1062)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:689)
	at org.apache.spark.sql.SparkSession.read(SparkSession.scala:645)
	at com.qf.bigdata.release.util.SparkHelper$.readTableData(SparkHelper.scala:25)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleReleaseJob(DWReleaseCustomer.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:109)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:107)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:107)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:124)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
Caused by: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	... 16 common frames omitted
Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	... 25 common frames omitted
Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:320)
	at org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.getProxy(ConfiguredFailoverProxyProvider.java:124)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:73)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:64)
	at org.apache.hadoop.io.retry.RetryProxy.create(RetryProxy.java:59)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:181)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:687)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:628)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2701)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2683)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:372)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:171)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:505)
	... 39 common frames omitted
Caused by: java.net.UnknownHostException: node242
	... 56 common frames omitted
17:32:48.017 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:32:48.050 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@3f3c966c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:32:48.066 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.182.1:4040
17:32:48.300 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:32:48.356 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:32:48.357 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:32:48.368 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:32:48.374 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:32:48.377 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:32:48.395 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:32:48.396 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\曹曹\AppData\Local\Temp\spark-86e9baf5-4961-4eea-af00-7976912a650b
17:35:10.243 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:35:10.802 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:35:10.832 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 曹曹,²ܲ
17:35:10.833 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 曹曹,²ܲ
17:35:10.834 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:35:10.835 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:35:10.836 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(曹曹, ²ܲ); groups with view permissions: Set(); users  with modify permissions: Set(曹曹, ²ܲ); groups with modify permissions: Set()
17:35:12.073 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5646.
17:35:12.164 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:35:12.205 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:35:12.212 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:35:12.212 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:35:12.229 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\曹曹\AppData\Local\Temp\blockmgr-a598ed1d-19f9-459c-a705-e22ec97bc4f1
17:35:12.258 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1994.1 MB
17:35:12.349 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:35:12.604 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @7649ms
17:35:12.903 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:35:12.981 INFO  [main] org.spark_project.jetty.server.Server - Started @8028ms
17:35:13.043 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6a175569{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:35:13.044 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:35:13.105 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10567255{/jobs,null,AVAILABLE,@Spark}
17:35:13.107 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d71adc2{/jobs/json,null,AVAILABLE,@Spark}
17:35:13.108 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a1d3c1a{/jobs/job,null,AVAILABLE,@Spark}
17:35:13.110 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57dc9128{/jobs/job/json,null,AVAILABLE,@Spark}
17:35:13.111 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17ae98d7{/stages,null,AVAILABLE,@Spark}
17:35:13.128 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac4944a{/stages/json,null,AVAILABLE,@Spark}
17:35:13.148 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39fc6b2c{/stages/stage,null,AVAILABLE,@Spark}
17:35:13.160 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cc9ce8{/stages/stage/json,null,AVAILABLE,@Spark}
17:35:13.166 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c0b41d6{/stages/pool,null,AVAILABLE,@Spark}
17:35:13.170 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bffa76d{/stages/pool/json,null,AVAILABLE,@Spark}
17:35:13.175 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/storage,null,AVAILABLE,@Spark}
17:35:13.178 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/storage/json,null,AVAILABLE,@Spark}
17:35:13.184 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/storage/rdd,null,AVAILABLE,@Spark}
17:35:13.185 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/storage/rdd/json,null,AVAILABLE,@Spark}
17:35:13.187 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/environment,null,AVAILABLE,@Spark}
17:35:13.190 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/environment/json,null,AVAILABLE,@Spark}
17:35:13.192 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5215cd9a{/executors,null,AVAILABLE,@Spark}
17:35:13.193 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/executors/json,null,AVAILABLE,@Spark}
17:35:13.194 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/executors/threadDump,null,AVAILABLE,@Spark}
17:35:13.195 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:35:13.230 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/static,null,AVAILABLE,@Spark}
17:35:13.233 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/,null,AVAILABLE,@Spark}
17:35:13.235 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6650813a{/api,null,AVAILABLE,@Spark}
17:35:13.236 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af1347d{/jobs/job/kill,null,AVAILABLE,@Spark}
17:35:13.238 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20765ed5{/stages/stage/kill,null,AVAILABLE,@Spark}
17:35:13.254 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.182.1:4040
17:35:16.267 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:35:16.396 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5667.
17:35:16.504 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.182.1:5667
17:35:16.625 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:35:16.703 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.182.1, 5667, None)
17:35:16.727 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.182.1:5667 with 1994.1 MB RAM, BlockManagerId(driver, 192.168.182.1, 5667, None)
17:35:16.759 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.182.1, 5667, None)
17:35:16.760 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.182.1, 5667, None)
17:35:17.028 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b022357{/metrics/json,null,AVAILABLE,@Spark}
17:35:24.195 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/release/target/classes/hive-site.xml
17:35:24.275 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('hdfs://hdfsCluster/hive/db').
17:35:24.276 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'hdfs://hdfsCluster/hive/db'.
17:35:24.294 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@261bd7b{/SQL,null,AVAILABLE,@Spark}
17:35:24.295 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@390037e7{/SQL/json,null,AVAILABLE,@Spark}
17:35:24.296 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2b08772d{/SQL/execution,null,AVAILABLE,@Spark}
17:35:24.296 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f171912{/SQL/execution/json,null,AVAILABLE,@Spark}
17:35:24.298 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b24087d{/static/sql,null,AVAILABLE,@Spark}
17:35:27.520 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:35:30.489 WARN  [main] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.metastore.local does not exist
17:35:30.894 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://Host01:9083
17:35:30.947 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for ²ܲ: Unknown error.
17:35:30.967 INFO  [main] hive.metastore - Connected to metastore.
17:35:43.364 WARN  [main] org.apache.hadoop.hdfs.DFSUtil - Namenode for hdfsCluster remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.
17:35:45.620 WARN  [main] org.apache.hadoop.hdfs.DFSUtil - Namenode for hdfsCluster remains unresolved for ID nn2.  Check your hdfs-site.xml file to ensure namenodes are configured properly.
17:35:45.781 ERROR [main] com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$ - Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1062)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:689)
	at org.apache.spark.sql.SparkSession.read(SparkSession.scala:645)
	at com.qf.bigdata.release.util.SparkHelper$.readTableData(SparkHelper.scala:25)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleReleaseJob(DWReleaseCustomer.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:109)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:107)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:107)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:124)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
Caused by: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	... 16 common frames omitted
Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	... 25 common frames omitted
Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:320)
	at org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.getProxy(ConfiguredFailoverProxyProvider.java:124)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:73)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:64)
	at org.apache.hadoop.io.retry.RetryProxy.create(RetryProxy.java:59)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:181)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:687)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:628)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2701)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2683)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:372)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:171)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:505)
	... 39 common frames omitted
Caused by: java.net.UnknownHostException: node242
	... 56 common frames omitted
17:35:45.822 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:35:45.903 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6a175569{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:35:45.928 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.182.1:4040
17:35:46.204 INFO  [dispatcher-event-loop-2] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:35:46.240 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:35:46.241 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:35:46.255 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:35:46.324 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:35:46.327 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:35:46.346 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:35:46.347 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\曹曹\AppData\Local\Temp\spark-68687a84-db0a-4507-9cd2-d4bdbf4e1188
17:37:06.507 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:37:06.969 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:37:07.034 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 曹曹,²ܲ
17:37:07.036 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 曹曹,²ܲ
17:37:07.038 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:37:07.040 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:37:07.041 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(曹曹, ²ܲ); groups with view permissions: Set(); users  with modify permissions: Set(曹曹, ²ܲ); groups with modify permissions: Set()
17:37:08.136 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5695.
17:37:08.168 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:37:08.193 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:37:08.197 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:37:08.198 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:37:08.209 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\曹曹\AppData\Local\Temp\blockmgr-db07e594-c428-4869-baa2-7ea213c2e82c
17:37:08.233 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1994.1 MB
17:37:08.298 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:37:08.445 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @6182ms
17:37:08.536 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:37:08.559 INFO  [main] org.spark_project.jetty.server.Server - Started @6298ms
17:37:08.591 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@6a175569{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:37:08.592 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:37:08.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@10567255{/jobs,null,AVAILABLE,@Spark}
17:37:08.632 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@d71adc2{/jobs/json,null,AVAILABLE,@Spark}
17:37:08.636 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a1d3c1a{/jobs/job,null,AVAILABLE,@Spark}
17:37:08.641 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@57dc9128{/jobs/job/json,null,AVAILABLE,@Spark}
17:37:08.643 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@17ae98d7{/stages,null,AVAILABLE,@Spark}
17:37:08.644 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ac4944a{/stages/json,null,AVAILABLE,@Spark}
17:37:08.645 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@39fc6b2c{/stages/stage,null,AVAILABLE,@Spark}
17:37:08.651 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7cc9ce8{/stages/stage/json,null,AVAILABLE,@Spark}
17:37:08.652 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@c0b41d6{/stages/pool,null,AVAILABLE,@Spark}
17:37:08.655 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bffa76d{/stages/pool/json,null,AVAILABLE,@Spark}
17:37:08.665 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d2260db{/storage,null,AVAILABLE,@Spark}
17:37:08.667 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49bf29c6{/storage/json,null,AVAILABLE,@Spark}
17:37:08.671 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3fcdcf{/storage/rdd,null,AVAILABLE,@Spark}
17:37:08.672 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46292372{/storage/rdd/json,null,AVAILABLE,@Spark}
17:37:08.682 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6c44052e{/environment,null,AVAILABLE,@Spark}
17:37:08.684 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@530a8454{/environment/json,null,AVAILABLE,@Spark}
17:37:08.690 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5215cd9a{/executors,null,AVAILABLE,@Spark}
17:37:08.693 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@31198ceb{/executors/json,null,AVAILABLE,@Spark}
17:37:08.695 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75201592{/executors/threadDump,null,AVAILABLE,@Spark}
17:37:08.698 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@aa5455e{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:37:08.711 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5dda14d0{/static,null,AVAILABLE,@Spark}
17:37:08.712 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@54e81b21{/,null,AVAILABLE,@Spark}
17:37:08.714 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6650813a{/api,null,AVAILABLE,@Spark}
17:37:08.715 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1af1347d{/jobs/job/kill,null,AVAILABLE,@Spark}
17:37:08.717 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@20765ed5{/stages/stage/kill,null,AVAILABLE,@Spark}
17:37:08.719 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.182.1:4040
17:37:08.962 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:37:09.010 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5716.
17:37:09.011 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.182.1:5716
17:37:09.013 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:37:09.072 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.182.1, 5716, None)
17:37:09.078 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.182.1:5716 with 1994.1 MB RAM, BlockManagerId(driver, 192.168.182.1, 5716, None)
17:37:09.082 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.182.1, 5716, None)
17:37:09.083 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.182.1, 5716, None)
17:37:09.388 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b022357{/metrics/json,null,AVAILABLE,@Spark}
17:37:11.975 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/release/target/classes/hive-site.xml
17:37:12.016 INFO  [main] org.apache.spark.sql.internal.SharedState - spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('hdfs://hdfsCluster/hive/db').
17:37:12.017 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'hdfs://hdfsCluster/hive/db'.
17:37:12.027 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@390037e7{/SQL,null,AVAILABLE,@Spark}
17:37:12.030 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f4596d0{/SQL/json,null,AVAILABLE,@Spark}
17:37:12.031 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f171912{/SQL/execution,null,AVAILABLE,@Spark}
17:37:12.033 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@159424e2{/SQL/execution/json,null,AVAILABLE,@Spark}
17:37:12.036 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4e245739{/static/sql,null,AVAILABLE,@Spark}
17:37:12.939 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:37:13.762 WARN  [main] org.apache.hadoop.hive.conf.HiveConf - HiveConf of name hive.metastore.local does not exist
17:37:13.996 INFO  [main] hive.metastore - Trying to connect to metastore with URI thrift://Host01:9083
17:37:14.036 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for ²ܲ: Unknown error.
17:37:14.054 INFO  [main] hive.metastore - Connected to metastore.
17:37:25.728 WARN  [main] org.apache.hadoop.hdfs.DFSUtil - Namenode for hdfsCluster remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.
17:37:27.986 WARN  [main] org.apache.hadoop.hdfs.DFSUtil - Namenode for hdfsCluster remains unresolved for ID nn2.  Check your hdfs-site.xml file to ensure namenodes are configured properly.
17:37:28.007 ERROR [main] com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$ - Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1062)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:689)
	at org.apache.spark.sql.SparkSession.read(SparkSession.scala:645)
	at com.qf.bigdata.release.util.SparkHelper$.readTableData(SparkHelper.scala:25)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleReleaseJob(DWReleaseCustomer.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:109)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:107)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:107)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:124)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
Caused by: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:105)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:93)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	... 16 common frames omitted
Caused by: java.lang.RuntimeException: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:191)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:362)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:266)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	... 25 common frames omitted
Caused by: java.lang.IllegalArgumentException: java.net.UnknownHostException: node242
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:378)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:320)
	at org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.getProxy(ConfiguredFailoverProxyProvider.java:124)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:73)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.<init>(RetryInvocationHandler.java:64)
	at org.apache.hadoop.io.retry.RetryProxy.create(RetryProxy.java:59)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:181)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:687)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:628)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:93)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2701)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2683)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:372)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:171)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:505)
	... 39 common frames omitted
Caused by: java.net.UnknownHostException: node242
	... 56 common frames omitted
17:37:28.016 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:37:28.022 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@6a175569{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:37:28.025 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.182.1:4040
17:37:28.149 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:37:28.162 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:37:28.163 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:37:28.164 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:37:28.171 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:37:28.174 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:37:28.176 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:37:28.178 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\曹曹\AppData\Local\Temp\spark-67ef6d45-e57c-41ed-87ec-964b1092b68a
17:48:26.461 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:48:27.152 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:48:27.182 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 曹曹,²ܲ
17:48:27.183 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 曹曹,²ܲ
17:48:27.184 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:48:27.184 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:48:27.185 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(曹曹, ²ܲ); groups with view permissions: Set(); users  with modify permissions: Set(曹曹, ²ܲ); groups with modify permissions: Set()
17:48:28.246 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5799.
17:48:28.290 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:48:28.321 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:48:28.325 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:48:28.326 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:48:28.343 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\曹曹\AppData\Local\Temp\blockmgr-439bbe67-3ba9-4200-880c-7220e225244d
17:48:28.385 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1994.1 MB
17:48:28.493 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:48:28.622 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @7723ms
17:48:28.720 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:48:28.741 INFO  [main] org.spark_project.jetty.server.Server - Started @7843ms
17:48:28.777 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@528c5039{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:48:28.777 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:48:28.812 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@214894fc{/jobs,null,AVAILABLE,@Spark}
17:48:28.813 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fb68ec6{/jobs/json,null,AVAILABLE,@Spark}
17:48:28.814 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3add81c4{/jobs/job,null,AVAILABLE,@Spark}
17:48:28.816 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@159e366{/jobs/job/json,null,AVAILABLE,@Spark}
17:48:28.817 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24528a25{/stages,null,AVAILABLE,@Spark}
17:48:28.818 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59221b97{/stages/json,null,AVAILABLE,@Spark}
17:48:28.819 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a772895{/stages/stage,null,AVAILABLE,@Spark}
17:48:28.822 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d332969{/stages/stage/json,null,AVAILABLE,@Spark}
17:48:28.823 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e27d72f{/stages/pool,null,AVAILABLE,@Spark}
17:48:28.824 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4837595f{/stages/pool/json,null,AVAILABLE,@Spark}
17:48:28.825 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/storage,null,AVAILABLE,@Spark}
17:48:28.827 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/storage/json,null,AVAILABLE,@Spark}
17:48:28.829 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ee55e70{/storage/rdd,null,AVAILABLE,@Spark}
17:48:28.830 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/storage/rdd/json,null,AVAILABLE,@Spark}
17:48:28.832 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/environment,null,AVAILABLE,@Spark}
17:48:28.833 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/environment/json,null,AVAILABLE,@Spark}
17:48:28.834 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/executors,null,AVAILABLE,@Spark}
17:48:28.835 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36b6964d{/executors/json,null,AVAILABLE,@Spark}
17:48:28.836 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/executors/threadDump,null,AVAILABLE,@Spark}
17:48:28.838 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:48:28.850 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/static,null,AVAILABLE,@Spark}
17:48:28.852 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/,null,AVAILABLE,@Spark}
17:48:28.855 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/api,null,AVAILABLE,@Spark}
17:48:28.856 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30457e14{/jobs/job/kill,null,AVAILABLE,@Spark}
17:48:28.860 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632aa1a3{/stages/stage/kill,null,AVAILABLE,@Spark}
17:48:28.864 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.182.1:4040
17:48:29.123 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:48:29.171 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5820.
17:48:29.172 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.182.1:5820
17:48:29.175 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:48:29.228 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.182.1, 5820, None)
17:48:29.233 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.182.1:5820 with 1994.1 MB RAM, BlockManagerId(driver, 192.168.182.1, 5820, None)
17:48:29.238 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.182.1, 5820, None)
17:48:29.239 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.182.1, 5820, None)
17:48:29.624 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@426e505c{/metrics/json,null,AVAILABLE,@Spark}
17:48:32.331 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/release/target/classes/hive-site.xml
17:48:32.462 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/release/spark-warehouse').
17:48:32.463 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/release/spark-warehouse'.
17:48:32.479 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2da81754{/SQL,null,AVAILABLE,@Spark}
17:48:32.481 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30bf26df{/SQL/json,null,AVAILABLE,@Spark}
17:48:32.483 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7f4596d0{/SQL/execution,null,AVAILABLE,@Spark}
17:48:32.484 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@588545ac{/SQL/execution/json,null,AVAILABLE,@Spark}
17:48:32.487 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29bcf51d{/static/sql,null,AVAILABLE,@Spark}
17:48:33.243 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:48:34.613 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17:48:34.716 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
17:48:37.390 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17:48:40.271 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
17:48:40.288 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
17:48:41.114 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
17:48:41.123 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
17:48:41.245 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
17:48:41.533 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
17:48:41.535 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_all_databases	
17:48:41.600 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=changjing pat=*
17:48:41.600 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=changjing pat=*	
17:48:41.711 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
17:48:41.712 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17:48:41.719 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
17:48:41.720 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
17:48:41.727 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dwd_release pat=*
17:48:41.728 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=dwd_release pat=*	
17:48:41.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=hive02 pat=*
17:48:41.733 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=hive02 pat=*	
17:48:41.740 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_hivedata pat=*
17:48:41.741 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=ods_hivedata pat=*	
17:48:41.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
17:48:41.747 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
17:48:41.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sql50 pat=*
17:48:41.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=sql50 pat=*	
17:48:41.760 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test0810 pat=*
17:48:41.761 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=test0810 pat=*	
17:48:41.767 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test83 pat=*
17:48:41.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=test83 pat=*	
17:48:41.773 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=weibo pat=*
17:48:41.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=weibo pat=*	
17:48:47.118 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ
17:48:47.217 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹
17:48:47.219 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/c4216fc8-19ad-47f8-8d4d-541aea1eeb0a_resources
17:48:47.227 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/c4216fc8-19ad-47f8-8d4d-541aea1eeb0a
17:48:47.231 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹/c4216fc8-19ad-47f8-8d4d-541aea1eeb0a
17:48:47.238 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/c4216fc8-19ad-47f8-8d4d-541aea1eeb0a/_tmp_space.db
17:48:47.279 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/release/spark-warehouse
17:48:47.346 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for ²ܲ: Unknown error.
17:48:47.435 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:48:47.435 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:48:47.455 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
17:48:47.456 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: global_temp	
17:48:47.460 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
17:48:47.921 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/f6f8eaa7-1d92-44ee-904e-dfdcbcf3c9a2_resources
17:48:47.930 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/f6f8eaa7-1d92-44ee-904e-dfdcbcf3c9a2
17:48:47.963 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹/f6f8eaa7-1d92-44ee-904e-dfdcbcf3c9a2
17:48:47.970 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/f6f8eaa7-1d92-44ee-904e-dfdcbcf3c9a2/_tmp_space.db
17:48:47.974 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/release/spark-warehouse
17:48:48.235 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
17:48:48.269 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
17:48:48.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:48:48.851 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:48:48.884 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:48.885 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:49.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:49.272 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:49.324 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.374 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.375 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.376 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.377 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.378 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:49.379 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:48:49.447 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
17:48:50.229 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table ods_release.ods_01_release_session (inference mode: INFER_AND_SAVE)
17:48:50.244 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:48:50.244 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:48:50.251 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:50.251 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:50.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:50.301 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:50.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.348 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.349 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.350 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.351 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.353 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.354 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.354 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.354 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:50.355 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:48:50.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=ods_release tbl=ods_01_release_session
17:48:50.646 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_partitions : db=ods_release tbl=ods_01_release_session	
17:48:52.922 INFO  [main] org.apache.spark.SparkContext - Starting job: table at SparkHelper.scala:25
17:48:53.054 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (table at SparkHelper.scala:25) with 1 output partitions
17:48:53.055 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (table at SparkHelper.scala:25)
17:48:53.056 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
17:48:53.058 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
17:48:53.089 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25), which has no missing parents
17:48:53.479 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 73.5 KB, free 1994.0 MB)
17:48:53.764 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.6 KB, free 1994.0 MB)
17:48:53.790 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.182.1:5820 (size: 26.6 KB, free: 1994.1 MB)
17:48:53.822 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1004
17:48:53.925 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at table at SparkHelper.scala:25) (first 15 tasks are for partitions Vector(0))
17:48:53.927 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
17:48:54.148 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4999 bytes)
17:48:54.189 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
17:48:56.003 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1781 bytes result sent to driver
17:48:56.038 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1928 ms on localhost (executor driver) (1/1)
17:48:56.048 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
17:48:56.089 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (table at SparkHelper.scala:25) finished in 2.038 s
17:48:56.124 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: table at SparkHelper.scala:25, took 3.200720 s
17:48:56.220 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table ods_release.ods_01_release_session
17:48:56.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:48:56.221 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:48:56.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:56.229 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:56.266 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:56.267 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:56.313 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.314 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.315 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.316 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.316 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.317 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.317 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.318 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.318 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.318 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:48:56.361 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:56.361 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:56.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:56.394 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:56.428 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.428 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.429 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.429 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.430 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.430 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.431 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.431 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.432 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:48:56.432 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:48:56.564 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:48:56.565 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:48:56.976 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=ods_release tbl=ods_01_release_session newtbl=ods_01_release_session
17:48:56.976 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=alter_table: db=ods_release tbl=ods_01_release_session newtbl=ods_01_release_session	
17:48:57.692 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
17:48:57.712 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
17:48:57.713 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
17:48:57.713 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
17:48:57.714 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
17:48:57.714 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
17:48:57.716 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. idcard') as idcard
17:48:57.763 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date _format(now(),' yyyy') as int)-cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as avg
17:48:57.787 ERROR [main] com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$ - 
no viable alternative at input '(cast(date _format'(line 1, pos 11)

== SQL ==
(cast(date _format(now(),' yyyy') as int)-cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as avg
-----------^^^

org.apache.spark.sql.catalyst.parser.ParseException: 
no viable alternative at input '(cast(date _format'(line 1, pos 11)

== SQL ==
(cast(date _format(now(),' yyyy') as int)-cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as avg
-----------^^^

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:217)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:114)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:43)
	at org.apache.spark.sql.Dataset$$anonfun$selectExpr$1.apply(Dataset.scala:1190)
	at org.apache.spark.sql.Dataset$$anonfun$selectExpr$1.apply(Dataset.scala:1189)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.qf.bigdata.release.util.SparkHelper$.readTableData(SparkHelper.scala:27)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleReleaseJob(DWReleaseCustomer.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:109)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:107)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:107)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:124)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
17:48:57.844 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:48:57.914 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@528c5039{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:48:58.002 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.182.1:4040
17:48:58.154 INFO  [dispatcher-event-loop-3] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:48:58.215 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:48:58.216 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:48:58.217 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:48:58.221 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:48:58.226 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:48:58.227 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:48:58.231 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\曹曹\AppData\Local\Temp\spark-6eacd046-125e-447b-b290-ad21a9d91226
17:53:48.194 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:53:48.967 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:53:49.004 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 曹曹,²ܲ
17:53:49.006 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 曹曹,²ܲ
17:53:49.007 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:53:49.009 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:53:49.011 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(曹曹, ²ܲ); groups with view permissions: Set(); users  with modify permissions: Set(曹曹, ²ܲ); groups with modify permissions: Set()
17:53:50.079 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5869.
17:53:50.104 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:53:50.128 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:53:50.132 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:53:50.132 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:53:50.143 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\曹曹\AppData\Local\Temp\blockmgr-cd7c9ccf-b245-450f-b1f7-5f3eac36c4c8
17:53:50.166 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1994.1 MB
17:53:50.228 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:53:50.340 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @6839ms
17:53:50.432 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:53:50.453 INFO  [main] org.spark_project.jetty.server.Server - Started @6953ms
17:53:50.482 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@686d1c8d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:53:50.483 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:53:50.521 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@214894fc{/jobs,null,AVAILABLE,@Spark}
17:53:50.523 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2fb68ec6{/jobs/json,null,AVAILABLE,@Spark}
17:53:50.524 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3add81c4{/jobs/job,null,AVAILABLE,@Spark}
17:53:50.526 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@159e366{/jobs/job/json,null,AVAILABLE,@Spark}
17:53:50.527 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24528a25{/stages,null,AVAILABLE,@Spark}
17:53:50.528 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59221b97{/stages/json,null,AVAILABLE,@Spark}
17:53:50.529 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a772895{/stages/stage,null,AVAILABLE,@Spark}
17:53:50.532 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d332969{/stages/stage/json,null,AVAILABLE,@Spark}
17:53:50.533 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e27d72f{/stages/pool,null,AVAILABLE,@Spark}
17:53:50.534 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4837595f{/stages/pool/json,null,AVAILABLE,@Spark}
17:53:50.536 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/storage,null,AVAILABLE,@Spark}
17:53:50.537 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/storage/json,null,AVAILABLE,@Spark}
17:53:50.542 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ee55e70{/storage/rdd,null,AVAILABLE,@Spark}
17:53:50.544 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/storage/rdd/json,null,AVAILABLE,@Spark}
17:53:50.554 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/environment,null,AVAILABLE,@Spark}
17:53:50.555 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/environment/json,null,AVAILABLE,@Spark}
17:53:50.556 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/executors,null,AVAILABLE,@Spark}
17:53:50.557 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36b6964d{/executors/json,null,AVAILABLE,@Spark}
17:53:50.566 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/executors/threadDump,null,AVAILABLE,@Spark}
17:53:50.569 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:53:50.579 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/static,null,AVAILABLE,@Spark}
17:53:50.580 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@27f0ad19{/,null,AVAILABLE,@Spark}
17:53:50.582 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/api,null,AVAILABLE,@Spark}
17:53:50.584 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@30457e14{/jobs/job/kill,null,AVAILABLE,@Spark}
17:53:50.585 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632aa1a3{/stages/stage/kill,null,AVAILABLE,@Spark}
17:53:50.589 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.182.1:4040
17:53:50.820 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:53:50.863 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5890.
17:53:50.864 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.182.1:5890
17:53:50.866 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:53:50.905 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.182.1, 5890, None)
17:53:50.911 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.182.1:5890 with 1994.1 MB RAM, BlockManagerId(driver, 192.168.182.1, 5890, None)
17:53:50.914 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.182.1, 5890, None)
17:53:50.915 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.182.1, 5890, None)
17:53:51.197 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@426e505c{/metrics/json,null,AVAILABLE,@Spark}
17:53:54.133 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/release/target/classes/hive-site.xml
17:53:54.171 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/release/spark-warehouse').
17:53:54.172 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/release/spark-warehouse'.
17:53:54.180 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f9e08d4{/SQL,null,AVAILABLE,@Spark}
17:53:54.181 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@704067c6{/SQL/json,null,AVAILABLE,@Spark}
17:53:54.182 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@78a8978a{/SQL/execution,null,AVAILABLE,@Spark}
17:53:54.183 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d4d8579{/SQL/execution/json,null,AVAILABLE,@Spark}
17:53:54.184 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@75fe1619{/static/sql,null,AVAILABLE,@Spark}
17:53:54.873 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:53:56.227 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17:53:56.273 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
17:53:57.751 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17:53:59.800 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
17:53:59.804 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
17:54:00.507 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
17:54:00.513 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
17:54:00.619 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
17:54:00.869 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
17:54:00.870 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_all_databases	
17:54:00.906 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=changjing pat=*
17:54:00.906 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=changjing pat=*	
17:54:01.018 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
17:54:01.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17:54:01.024 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
17:54:01.024 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
17:54:01.030 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dwd_release pat=*
17:54:01.030 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=dwd_release pat=*	
17:54:01.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=hive02 pat=*
17:54:01.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=hive02 pat=*	
17:54:01.044 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_hivedata pat=*
17:54:01.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=ods_hivedata pat=*	
17:54:01.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
17:54:01.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
17:54:01.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sql50 pat=*
17:54:01.057 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=sql50 pat=*	
17:54:01.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test0810 pat=*
17:54:01.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=test0810 pat=*	
17:54:01.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test83 pat=*
17:54:01.068 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=test83 pat=*	
17:54:01.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=weibo pat=*
17:54:01.075 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=weibo pat=*	
17:54:06.468 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/7beb10f6-c0c3-4ef8-ac3e-044fe6dd6ce9_resources
17:54:06.491 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/7beb10f6-c0c3-4ef8-ac3e-044fe6dd6ce9
17:54:06.494 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹/7beb10f6-c0c3-4ef8-ac3e-044fe6dd6ce9
17:54:06.501 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/7beb10f6-c0c3-4ef8-ac3e-044fe6dd6ce9/_tmp_space.db
17:54:06.514 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/release/spark-warehouse
17:54:06.604 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for ²ܲ: Unknown error.
17:54:06.703 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:54:06.704 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:54:06.749 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
17:54:06.750 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: global_temp	
17:54:06.754 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
17:54:07.502 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/f4a67b21-77ed-4bcb-9ec6-a1c2a614cb59_resources
17:54:07.510 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/f4a67b21-77ed-4bcb-9ec6-a1c2a614cb59
17:54:07.514 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹/f4a67b21-77ed-4bcb-9ec6-a1c2a614cb59
17:54:07.521 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/f4a67b21-77ed-4bcb-9ec6-a1c2a614cb59/_tmp_space.db
17:54:07.525 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/release/spark-warehouse
17:54:07.717 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
17:54:07.818 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
17:54:08.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:54:08.530 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:54:08.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:54:08.538 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:54:08.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:54:08.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:54:08.989 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.041 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.042 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.043 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.044 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:54:09.044 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:54:09.122 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
17:54:10.886 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
17:54:10.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
17:54:10.917 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
17:54:10.919 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
17:54:10.929 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
17:54:10.931 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
17:54:10.938 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. idcard') as idcard
17:54:11.046 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),' yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as avg
17:54:11.122 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
17:54:11.126 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. area_code') as area_code
17:54:11.128 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. Longitude') as longitude
17:54:11.129 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. latitude') as latitude
17:54:11.130 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. matter _id') as matter_id
17:54:11.131 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. model _code') as modelcode
17:54:11.134 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. model version') as model _version
17:54:11.145 ERROR [main] com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$ - 
extraneous input '_version' expecting <EOF>(line 1, pos 50)

== SQL ==
get_json_object(exts,'$. model version') as model _version
--------------------------------------------------^^^

org.apache.spark.sql.catalyst.parser.ParseException: 
extraneous input '_version' expecting <EOF>(line 1, pos 50)

== SQL ==
get_json_object(exts,'$. model version') as model _version
--------------------------------------------------^^^

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:217)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:114)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:43)
	at org.apache.spark.sql.Dataset$$anonfun$selectExpr$1.apply(Dataset.scala:1190)
	at org.apache.spark.sql.Dataset$$anonfun$selectExpr$1.apply(Dataset.scala:1189)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1189)
	at com.qf.bigdata.release.util.SparkHelper$.readTableData(SparkHelper.scala:27)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleReleaseJob(DWReleaseCustomer.scala:51)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:109)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$$anonfun$handleJobs$1.apply(DWReleaseCustomer.scala:107)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.handleJobs(DWReleaseCustomer.scala:107)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer$.main(DWReleaseCustomer.scala:124)
	at com.qf.bigdata.release.etl.release.dw.DWReleaseCustomer.main(DWReleaseCustomer.scala)
17:54:11.212 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:54:11.271 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@686d1c8d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:54:11.286 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.182.1:4040
17:54:11.391 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:54:11.551 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:54:11.557 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:54:11.558 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:54:11.590 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:54:11.600 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:54:11.601 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:54:11.613 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\曹曹\AppData\Local\Temp\spark-b97a3138-877a-40b7-90e0-ba8e3dfff0bc
17:58:01.821 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
17:58:03.189 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
17:58:03.357 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 曹曹,²ܲ
17:58:03.359 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 曹曹,²ܲ
17:58:03.392 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
17:58:03.394 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
17:58:03.399 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(曹曹, ²ܲ); groups with view permissions: Set(); users  with modify permissions: Set(曹曹, ²ܲ); groups with modify permissions: Set()
17:58:05.271 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 5960.
17:58:05.357 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
17:58:05.466 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
17:58:05.487 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17:58:05.488 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
17:58:05.652 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\曹曹\AppData\Local\Temp\blockmgr-cfe023d2-c457-4630-adf7-affc11a283f8
17:58:05.805 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1994.1 MB
17:58:05.955 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
17:58:06.319 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @15112ms
17:58:06.601 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
17:58:06.675 INFO  [main] org.spark_project.jetty.server.Server - Started @15470ms
17:58:06.753 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@238d0718{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:58:06.753 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
17:58:06.801 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e362c57{/jobs,null,AVAILABLE,@Spark}
17:58:06.802 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3add81c4{/jobs/json,null,AVAILABLE,@Spark}
17:58:06.803 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c65121{/jobs/job,null,AVAILABLE,@Spark}
17:58:06.807 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24528a25{/jobs/job/json,null,AVAILABLE,@Spark}
17:58:06.808 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59221b97{/stages,null,AVAILABLE,@Spark}
17:58:06.810 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a772895{/stages/json,null,AVAILABLE,@Spark}
17:58:06.812 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@704b2127{/stages/stage,null,AVAILABLE,@Spark}
17:58:06.816 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e27d72f{/stages/stage/json,null,AVAILABLE,@Spark}
17:58:06.820 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4837595f{/stages/pool,null,AVAILABLE,@Spark}
17:58:06.823 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/stages/pool/json,null,AVAILABLE,@Spark}
17:58:06.828 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/storage,null,AVAILABLE,@Spark}
17:58:06.829 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ee55e70{/storage/json,null,AVAILABLE,@Spark}
17:58:06.830 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/storage/rdd,null,AVAILABLE,@Spark}
17:58:06.838 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/storage/rdd/json,null,AVAILABLE,@Spark}
17:58:06.839 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/environment,null,AVAILABLE,@Spark}
17:58:06.840 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/environment/json,null,AVAILABLE,@Spark}
17:58:06.841 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36b6964d{/executors,null,AVAILABLE,@Spark}
17:58:06.844 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/executors/json,null,AVAILABLE,@Spark}
17:58:06.846 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/executors/threadDump,null,AVAILABLE,@Spark}
17:58:06.847 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/executors/threadDump/json,null,AVAILABLE,@Spark}
17:58:06.861 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/static,null,AVAILABLE,@Spark}
17:58:06.870 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/,null,AVAILABLE,@Spark}
17:58:06.872 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/api,null,AVAILABLE,@Spark}
17:58:06.874 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632aa1a3{/jobs/job/kill,null,AVAILABLE,@Spark}
17:58:06.875 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b582111{/stages/stage/kill,null,AVAILABLE,@Spark}
17:58:06.883 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.182.1:4040
17:58:07.417 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
17:58:07.556 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5981.
17:58:07.564 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.182.1:5981
17:58:07.567 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17:58:07.642 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.182.1, 5981, None)
17:58:07.672 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.182.1:5981 with 1994.1 MB RAM, BlockManagerId(driver, 192.168.182.1, 5981, None)
17:58:07.685 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.182.1, 5981, None)
17:58:07.686 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.182.1, 5981, None)
17:58:08.289 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f8e0cee{/metrics/json,null,AVAILABLE,@Spark}
17:58:12.479 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/release/target/classes/hive-site.xml
17:58:12.567 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/release/spark-warehouse').
17:58:12.568 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/release/spark-warehouse'.
17:58:12.628 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@704067c6{/SQL,null,AVAILABLE,@Spark}
17:58:12.628 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6002e944{/SQL/json,null,AVAILABLE,@Spark}
17:58:12.629 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d4d8579{/SQL/execution,null,AVAILABLE,@Spark}
17:58:12.630 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29bcf51d{/SQL/execution/json,null,AVAILABLE,@Spark}
17:58:12.656 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d66e944{/static/sql,null,AVAILABLE,@Spark}
17:58:15.005 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17:58:16.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17:58:16.862 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
17:58:19.077 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17:58:22.063 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
17:58:22.184 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
17:58:23.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
17:58:23.063 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
17:58:23.194 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
17:58:23.415 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
17:58:23.417 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_all_databases	
17:58:23.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=changjing pat=*
17:58:23.440 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=changjing pat=*	
17:58:23.544 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
17:58:23.545 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17:58:23.550 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
17:58:23.551 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
17:58:23.556 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dwd_release pat=*
17:58:23.557 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=dwd_release pat=*	
17:58:23.562 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=hive02 pat=*
17:58:23.563 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=hive02 pat=*	
17:58:23.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_hivedata pat=*
17:58:23.568 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=ods_hivedata pat=*	
17:58:23.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
17:58:23.573 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
17:58:23.579 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sql50 pat=*
17:58:23.580 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=sql50 pat=*	
17:58:23.584 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test0810 pat=*
17:58:23.585 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=test0810 pat=*	
17:58:23.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test83 pat=*
17:58:23.590 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=test83 pat=*	
17:58:23.595 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=weibo pat=*
17:58:23.596 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=weibo pat=*	
17:58:27.954 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/573fa504-1b98-4ebf-ac8c-b90852d4ec18_resources
17:58:27.973 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/573fa504-1b98-4ebf-ac8c-b90852d4ec18
17:58:27.979 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹/573fa504-1b98-4ebf-ac8c-b90852d4ec18
17:58:27.988 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/573fa504-1b98-4ebf-ac8c-b90852d4ec18/_tmp_space.db
17:58:27.998 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/release/spark-warehouse
17:58:28.046 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for ²ܲ: Unknown error.
17:58:28.091 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:28.092 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:28.110 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
17:58:28.110 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: global_temp	
17:58:28.120 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
17:58:28.538 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/7868bd46-9581-4d29-a8f3-84e93fac16ec_resources
17:58:28.544 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/7868bd46-9581-4d29-a8f3-84e93fac16ec
17:58:28.547 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹/7868bd46-9581-4d29-a8f3-84e93fac16ec
17:58:28.563 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/7868bd46-9581-4d29-a8f3-84e93fac16ec/_tmp_space.db
17:58:28.566 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/release/spark-warehouse
17:58:28.703 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
17:58:28.719 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
17:58:29.463 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:58:29.464 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:58:29.471 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:58:29.472 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:58:29.696 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:58:29.697 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:58:29.740 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.765 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.765 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.766 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.766 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.767 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.768 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:29.769 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:29.801 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
17:58:30.847 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
17:58:30.866 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
17:58:30.866 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
17:58:30.868 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
17:58:30.869 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
17:58:30.869 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
17:58:30.870 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. idcard') as idcard
17:58:30.906 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),' yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as avg
17:58:30.955 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
17:58:30.959 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. area_code') as area_code
17:58:30.961 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. Longitude') as longitude
17:58:30.962 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. latitude') as latitude
17:58:30.963 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. matter_id') as matter_id
17:58:30.965 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. model_code') as model_code
17:58:30.966 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. model_version') as model_version
17:58:30.968 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. aid') as aid
17:58:30.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
17:58:30.969 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
17:58:30.980 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:30.980 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:30.988 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:30.989 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:30.995 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:30.996 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.004 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.004 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.012 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.020 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.033 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.040 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.040 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.072 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.073 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.081 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.082 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.096 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.097 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:31.106 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
17:58:31.107 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
17:58:32.299 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:58:32.299 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:58:32.308 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:58:32.309 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:58:32.360 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:58:32.361 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:58:32.407 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.408 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.408 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.409 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.409 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.409 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.409 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.410 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.410 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:32.410 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:32.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:58:32.498 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:58:32.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:58:32.523 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:58:32.991 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:58:32.994 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
17:58:32.996 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
17:58:33.056 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
17:58:33.072 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
17:58:34.209 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 474.6166 ms
17:58:34.788 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 102.4183 ms
17:58:35.311 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 303.2 KB, free 1993.8 MB)
17:58:35.607 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.8 KB, free 1993.8 MB)
17:58:35.614 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.182.1:5981 (size: 25.8 KB, free: 1994.1 MB)
17:58:35.654 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:58
17:58:35.709 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17:58:36.038 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:58
17:58:36.117 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:58)
17:58:36.121 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:58) with 1 output partitions
17:58:36.122 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:58)
17:58:36.122 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
17:58:36.125 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
17:58:36.138 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:58), which has no missing parents
17:58:36.246 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 1993.8 MB)
17:58:36.252 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1993.8 MB)
17:58:36.258 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.182.1:5981 (size: 2.2 KB, free: 1994.1 MB)
17:58:36.259 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
17:58:36.300 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:58) (first 15 tasks are for partitions Vector(0))
17:58:36.301 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
17:58:36.426 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
17:58:36.486 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 0)
17:58:36.751 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
17:58:36.759 INFO  [Executor task launch worker for task 0] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 19 ms
17:58:36.847 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 0). 1225 bytes result sent to driver
17:58:36.876 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 0) in 465 ms on localhost (executor driver) (1/1)
17:58:36.892 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
17:58:36.971 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:58) finished in 0.537 s
17:58:36.992 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:58, took 0.953610 s
17:58:37.042 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:58
17:58:37.043 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (show at DWReleaseCustomer.scala:58) with 3 output partitions
17:58:37.044 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (show at DWReleaseCustomer.scala:58)
17:58:37.044 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
17:58:37.044 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
17:58:37.045 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:58), which has no missing parents
17:58:37.048 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1993.8 MB)
17:58:37.070 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1993.8 MB)
17:58:37.074 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.182.1:5981 (size: 2.2 KB, free: 1994.1 MB)
17:58:37.074 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
17:58:37.076 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 3 missing tasks from ResultStage 3 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:58) (first 15 tasks are for partitions Vector(1, 2, 3))
17:58:37.076 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 3 tasks
17:58:37.078 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
17:58:37.079 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
17:58:37.079 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
17:58:37.081 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 1)
17:58:37.083 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 3)
17:58:37.083 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 2)
17:58:37.099 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
17:58:37.099 INFO  [Executor task launch worker for task 1] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:58:37.102 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
17:58:37.102 INFO  [Executor task launch worker for task 3] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
17:58:37.103 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
17:58:37.103 INFO  [Executor task launch worker for task 2] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:58:37.104 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 3). 1225 bytes result sent to driver
17:58:37.105 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 2). 1225 bytes result sent to driver
17:58:37.106 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 1). 1182 bytes result sent to driver
17:58:37.113 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 1) in 36 ms on localhost (executor driver) (1/3)
17:58:37.114 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 2) in 36 ms on localhost (executor driver) (2/3)
17:58:37.115 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 3) in 36 ms on localhost (executor driver) (3/3)
17:58:37.115 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
17:58:37.116 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (show at DWReleaseCustomer.scala:58) finished in 0.039 s
17:58:37.117 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: show at DWReleaseCustomer.scala:58, took 0.074894 s
17:58:37.203 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dwd_release.dw_release_customer
17:58:37.241 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:37.242 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:37.284 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.285 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.285 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.285 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.286 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.286 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.286 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.288 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.289 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:58:37.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.290 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.291 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.291 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.292 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.293 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:37.293 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:37.533 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_17-58-37_526_5652415918769258557-1
17:58:38.043 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
17:58:38.043 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
17:58:38.067 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:58:38.067 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:58:38.155 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
17:58:38.155 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
17:58:38.199 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.200 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.201 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.202 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.202 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.203 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:38.203 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:38.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
17:58:38.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
17:58:38.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
17:58:38.209 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
17:58:38.254 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 20190924)
17:58:38.258 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
17:58:38.260 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
17:58:38.261 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
17:58:38.263 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 0 partitions out of 0, pruned 0 partitions.
17:58:38.375 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:58:38.377 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:58:38.518 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 67.8926 ms
17:58:38.551 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 303.2 KB, free 1993.5 MB)
17:58:38.597 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.8 KB, free 1993.4 MB)
17:58:38.598 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.182.1:5981 (size: 25.8 KB, free: 1994.0 MB)
17:58:38.601 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:36
17:58:38.601 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
17:58:38.789 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:36
17:58:38.792 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:36)
17:58:38.792 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 2 (insertInto at SparkHelper.scala:36) with 4 output partitions
17:58:38.792 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (insertInto at SparkHelper.scala:36)
17:58:38.793 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
17:58:38.793 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
17:58:38.794 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:36), which has no missing parents
17:58:38.945 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 177.6 KB, free 1993.3 MB)
17:58:38.950 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 65.1 KB, free 1993.2 MB)
17:58:38.961 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.182.1:5981 (size: 65.1 KB, free: 1994.0 MB)
17:58:38.962 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
17:58:38.963 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
17:58:38.963 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 4 tasks
17:58:38.965 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
17:58:38.965 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
17:58:38.966 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
17:58:38.966 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
17:58:38.967 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 4)
17:58:38.967 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 5)
17:58:38.967 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 2.0 in stage 5.0 (TID 6)
17:58:39.063 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 3.0 in stage 5.0 (TID 7)
17:58:39.104 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
17:58:39.105 INFO  [Executor task launch worker for task 5] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
17:58:39.122 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
17:58:39.123 INFO  [Executor task launch worker for task 7] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
17:58:39.134 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
17:58:39.134 INFO  [Executor task launch worker for task 6] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:58:39.145 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 0 blocks
17:58:39.145 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
17:58:39.494 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
17:58:39.495 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
17:58:39.501 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 359.4116 ms
17:58:39.519 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
17:58:39.519 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
17:58:39.520 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
17:58:39.520 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
17:58:39.520 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
17:58:39.520 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
17:58:39.616 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.182.1:5981 in memory (size: 25.8 KB, free: 1994.0 MB)
17:58:39.661 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.182.1:5981 in memory (size: 2.2 KB, free: 1994.0 MB)
17:58:39.662 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
17:58:39.662 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
17:58:39.666 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.182.1:5981 in memory (size: 2.2 KB, free: 1994.0 MB)
17:58:39.667 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
17:58:39.702 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 22.5843 ms
17:58:39.784 INFO  [Executor task launch worker for task 7] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:58:39.784 INFO  [Executor task launch worker for task 4] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:58:39.784 INFO  [Executor task launch worker for task 5] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:58:39.785 INFO  [Executor task launch worker for task 6] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
17:58:39.786 INFO  [Executor task launch worker for task 4] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:58:39.786 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:58:39.787 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:58:39.788 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
17:58:39.823 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.6645 ms
17:58:39.902 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 41.9555 ms
17:58:39.957 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 49.4838 ms
17:58:39.985 INFO  [Executor task launch worker for task 5] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925175839_0005_m_000001_0
17:58:39.987 INFO  [Executor task launch worker for task 4] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925175839_0005_m_000000_0
17:58:39.992 INFO  [Executor task launch worker for task 6] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925175839_0005_m_000002_0
17:58:39.994 INFO  [Executor task launch worker for task 7] org.apache.spark.mapred.SparkHadoopMapRedUtil - No need to commit output of task because needsTaskCommit=false: attempt_20190925175839_0005_m_000003_0
17:58:40.003 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 4). 2631 bytes result sent to driver
17:58:40.004 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 3.0 in stage 5.0 (TID 7). 2631 bytes result sent to driver
17:58:40.007 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 2.0 in stage 5.0 (TID 6). 2588 bytes result sent to driver
17:58:40.009 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 4) in 1045 ms on localhost (executor driver) (1/4)
17:58:40.011 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 6) in 1045 ms on localhost (executor driver) (2/4)
17:58:40.011 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 7) in 1045 ms on localhost (executor driver) (3/4)
17:58:40.024 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 5). 2631 bytes result sent to driver
17:58:40.026 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 5) in 1061 ms on localhost (executor driver) (4/4)
17:58:40.026 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
17:58:40.028 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (insertInto at SparkHelper.scala:36) finished in 1.063 s
17:58:40.028 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 2 finished: insertInto at SparkHelper.scala:36, took 1.237645 s
17:58:40.318 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
17:58:40.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:40.320 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:40.352 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:40.353 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:40.384 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.384 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.384 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.385 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.385 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.385 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.386 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.386 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.387 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:58:40.387 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.387 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.387 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.388 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.388 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.388 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.389 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:40.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:40.393 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:40.460 WARN  [main] hive.ql.metadata.Hive - No partition is generated by dynamic partitioning
17:58:40.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:40.490 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:40.572 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dwd_release`.`dw_release_customer`
17:58:40.577 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dwd_release
17:58:40.578 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: dwd_release	
17:58:40.587 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:40.587 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:40.648 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:40.649 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:40.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.691 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.692 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.692 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.692 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.693 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.693 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.693 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:58:40.693 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.694 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.695 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:40.705 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Inferring case-sensitive schema for table dwd_release.dw_release_customer (inference mode: INFER_AND_SAVE)
17:58:40.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dwd_release
17:58:40.707 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: dwd_release	
17:58:40.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:40.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:40.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:40.774 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:40.815 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.815 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.816 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:58:40.817 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.818 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.819 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.819 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:40.819 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:40.821 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions : db=dwd_release tbl=dw_release_customer
17:58:40.822 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_partitions : db=dwd_release tbl=dw_release_customer	
17:58:41.369 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:36
17:58:41.370 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 3 (insertInto at SparkHelper.scala:36) with 1 output partitions
17:58:41.370 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (insertInto at SparkHelper.scala:36)
17:58:41.370 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
17:58:41.370 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
17:58:41.371 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:36), which has no missing parents
17:58:41.404 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 73.5 KB, free 1993.5 MB)
17:58:41.415 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 26.6 KB, free 1993.4 MB)
17:58:41.426 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.182.1:5981 (size: 26.6 KB, free: 1994.0 MB)
17:58:41.427 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
17:58:41.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[16] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0))
17:58:41.435 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 1 tasks
17:58:41.442 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 4987 bytes)
17:58:41.443 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 8)
17:58:42.685 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 8). 2064 bytes result sent to driver
17:58:42.686 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 8) in 1251 ms on localhost (executor driver) (1/1)
17:58:42.687 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
17:58:42.687 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (insertInto at SparkHelper.scala:36) finished in 1.246 s
17:58:42.688 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 3 finished: insertInto at SparkHelper.scala:36, took 1.318976 s
17:58:42.721 INFO  [main] org.apache.spark.sql.hive.HiveMetastoreCatalog - Saving case-sensitive schema for table dwd_release.dw_release_customer
17:58:42.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dwd_release
17:58:42.723 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: dwd_release	
17:58:42.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:42.729 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:42.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:42.757 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:42.789 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.789 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.790 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.790 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.791 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:58:42.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.792 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.793 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.794 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:42.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:42.831 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:42.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:42.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:42.900 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.903 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
17:58:42.904 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.905 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.905 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.905 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.905 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
17:58:42.906 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
17:58:42.933 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
17:58:42.935 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
17:58:43.202 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_table: db=dwd_release tbl=dw_release_customer newtbl=dw_release_customer
17:58:43.203 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=alter_table: db=dwd_release tbl=dw_release_customer newtbl=dw_release_customer	
17:58:43.457 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
17:58:43.495 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@238d0718{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
17:58:43.510 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.182.1:4040
17:58:43.584 INFO  [dispatcher-event-loop-0] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
17:58:43.704 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
17:58:43.714 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
17:58:43.725 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
17:58:43.743 INFO  [dispatcher-event-loop-2] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
17:58:43.747 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
17:58:43.748 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
17:58:43.749 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\曹曹\AppData\Local\Temp\spark-929e1868-d615-4791-a98d-80b93f7e1f69
18:02:25.059 INFO  [main] org.apache.spark.SparkContext - Running Spark version 2.2.3
18:02:26.458 INFO  [main] org.apache.spark.SparkContext - Submitted application: dw_release_job
18:02:26.581 INFO  [main] org.apache.spark.SecurityManager - Changing view acls to: 曹曹,²ܲ
18:02:26.583 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls to: 曹曹,²ܲ
18:02:26.627 INFO  [main] org.apache.spark.SecurityManager - Changing view acls groups to: 
18:02:26.628 INFO  [main] org.apache.spark.SecurityManager - Changing modify acls groups to: 
18:02:26.634 INFO  [main] org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(曹曹, ²ܲ); groups with view permissions: Set(); users  with modify permissions: Set(曹曹, ²ܲ); groups with modify permissions: Set()
18:02:28.129 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 6026.
18:02:28.204 INFO  [main] org.apache.spark.SparkEnv - Registering MapOutputTracker
18:02:28.280 INFO  [main] org.apache.spark.SparkEnv - Registering BlockManagerMaster
18:02:28.290 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18:02:28.291 INFO  [main] org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
18:02:28.322 INFO  [main] org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\曹曹\AppData\Local\Temp\blockmgr-7083edfa-d578-47f7-ae79-bb14082ab52e
18:02:28.410 INFO  [main] org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1994.1 MB
18:02:28.557 INFO  [main] org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
18:02:28.799 INFO  [main] org.spark_project.jetty.util.log - Logging initialized @11988ms
18:02:29.023 INFO  [main] org.spark_project.jetty.server.Server - jetty-9.3.z-SNAPSHOT
18:02:29.069 INFO  [main] org.spark_project.jetty.server.Server - Started @12259ms
18:02:29.114 INFO  [main] org.spark_project.jetty.server.AbstractConnector - Started ServerConnector@792f2a8f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18:02:29.115 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
18:02:29.155 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@e362c57{/jobs,null,AVAILABLE,@Spark}
18:02:29.156 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3add81c4{/jobs/json,null,AVAILABLE,@Spark}
18:02:29.157 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c65121{/jobs/job,null,AVAILABLE,@Spark}
18:02:29.159 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24528a25{/jobs/job/json,null,AVAILABLE,@Spark}
18:02:29.160 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@59221b97{/stages,null,AVAILABLE,@Spark}
18:02:29.161 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5a772895{/stages/json,null,AVAILABLE,@Spark}
18:02:29.162 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@704b2127{/stages/stage,null,AVAILABLE,@Spark}
18:02:29.165 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2e27d72f{/stages/stage/json,null,AVAILABLE,@Spark}
18:02:29.166 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4837595f{/stages/pool,null,AVAILABLE,@Spark}
18:02:29.167 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b718392{/stages/pool/json,null,AVAILABLE,@Spark}
18:02:29.168 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1f2d2181{/storage,null,AVAILABLE,@Spark}
18:02:29.169 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7ee55e70{/storage/json,null,AVAILABLE,@Spark}
18:02:29.171 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7668d560{/storage/rdd,null,AVAILABLE,@Spark}
18:02:29.172 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@126be319{/storage/rdd/json,null,AVAILABLE,@Spark}
18:02:29.174 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5c371e13{/environment,null,AVAILABLE,@Spark}
18:02:29.175 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1e34c607{/environment/json,null,AVAILABLE,@Spark}
18:02:29.176 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@36b6964d{/executors,null,AVAILABLE,@Spark}
18:02:29.178 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@9257031{/executors/json,null,AVAILABLE,@Spark}
18:02:29.179 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7726e185{/executors/threadDump,null,AVAILABLE,@Spark}
18:02:29.181 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@282308c3{/executors/threadDump/json,null,AVAILABLE,@Spark}
18:02:29.203 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1db0ec27{/static,null,AVAILABLE,@Spark}
18:02:29.204 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@38d5b107{/,null,AVAILABLE,@Spark}
18:02:29.206 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@44ea608c{/api,null,AVAILABLE,@Spark}
18:02:29.208 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@632aa1a3{/jobs/job/kill,null,AVAILABLE,@Spark}
18:02:29.209 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3b582111{/stages/stage/kill,null,AVAILABLE,@Spark}
18:02:29.222 INFO  [main] org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.182.1:4040
18:02:29.588 INFO  [main] org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
18:02:29.725 INFO  [main] org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6047.
18:02:29.726 INFO  [main] org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.182.1:6047
18:02:29.729 INFO  [main] org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18:02:29.790 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.182.1, 6047, None)
18:02:29.796 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.182.1:6047 with 1994.1 MB RAM, BlockManagerId(driver, 192.168.182.1, 6047, None)
18:02:29.812 INFO  [main] org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.182.1, 6047, None)
18:02:29.813 INFO  [main] org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.182.1, 6047, None)
18:02:30.187 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6f8e0cee{/metrics/json,null,AVAILABLE,@Spark}
18:02:33.405 INFO  [main] org.apache.spark.sql.internal.SharedState - loading hive config file: file:/E:/release/target/classes/hive-site.xml
18:02:33.456 INFO  [main] org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/release/spark-warehouse').
18:02:33.457 INFO  [main] org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/E:/release/spark-warehouse'.
18:02:33.480 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@704067c6{/SQL,null,AVAILABLE,@Spark}
18:02:33.481 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6002e944{/SQL/json,null,AVAILABLE,@Spark}
18:02:33.482 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7d4d8579{/SQL/execution,null,AVAILABLE,@Spark}
18:02:33.483 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29bcf51d{/SQL/execution/json,null,AVAILABLE,@Spark}
18:02:33.487 INFO  [main] org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d66e944{/static/sql,null,AVAILABLE,@Spark}
18:02:34.270 INFO  [main] org.apache.spark.sql.hive.HiveUtils - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
18:02:35.342 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
18:02:35.382 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - ObjectStore, initialize called
18:02:37.071 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
18:02:39.198 INFO  [main] org.apache.hadoop.hive.metastore.MetaStoreDirectSql - Using direct SQL, underlying DB is MYSQL
18:02:39.201 INFO  [main] org.apache.hadoop.hive.metastore.ObjectStore - Initialized ObjectStore
18:02:39.677 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added admin role in metastore
18:02:39.683 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - Added public role in metastore
18:02:39.772 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - No user is added in admin role, since config is empty
18:02:39.926 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_all_databases
18:02:39.928 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_all_databases	
18:02:39.947 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=changjing pat=*
18:02:39.947 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=changjing pat=*	
18:02:40.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=default pat=*
18:02:40.035 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
18:02:40.039 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dm_release pat=*
18:02:40.040 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=dm_release pat=*	
18:02:40.044 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=dwd_release pat=*
18:02:40.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=dwd_release pat=*	
18:02:40.050 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=hive02 pat=*
18:02:40.051 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=hive02 pat=*	
18:02:40.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_hivedata pat=*
18:02:40.055 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=ods_hivedata pat=*	
18:02:40.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=ods_release pat=*
18:02:40.060 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=ods_release pat=*	
18:02:40.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=sql50 pat=*
18:02:40.065 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=sql50 pat=*	
18:02:40.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test0810 pat=*
18:02:40.070 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=test0810 pat=*	
18:02:40.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=test83 pat=*
18:02:40.074 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=test83 pat=*	
18:02:40.079 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_functions: db=weibo pat=*
18:02:40.080 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_functions: db=weibo pat=*	
18:02:40.892 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/703c1181-e72c-4697-94fb-1c7e50780632_resources
18:02:40.903 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/703c1181-e72c-4697-94fb-1c7e50780632
18:02:40.906 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹/703c1181-e72c-4697-94fb-1c7e50780632
18:02:40.928 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/703c1181-e72c-4697-94fb-1c7e50780632/_tmp_space.db
18:02:40.933 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/release/spark-warehouse
18:02:40.947 INFO  [main] org.apache.hadoop.security.JniBasedUnixGroupsMapping - Error getting groups for ²ܲ: Unknown error.
18:02:40.962 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:40.963 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:40.972 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: global_temp
18:02:40.973 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: global_temp	
18:02:40.977 WARN  [main] org.apache.hadoop.hive.metastore.ObjectStore - Failed to get database global_temp, returning NoSuchObjectException
18:02:41.329 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/4a7e0d14-0e03-493e-a702-48e6aa96f7f5_resources
18:02:41.335 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/4a7e0d14-0e03-493e-a702-48e6aa96f7f5
18:02:41.341 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created local directory: C:/Users/曹曹/AppData/Local/Temp/曹曹/4a7e0d14-0e03-493e-a702-48e6aa96f7f5
18:02:41.349 INFO  [main] org.apache.hadoop.hive.ql.session.SessionState - Created HDFS directory: /tmp/hive/²ܲ/4a7e0d14-0e03-493e-a702-48e6aa96f7f5/_tmp_space.db
18:02:41.352 INFO  [main] org.apache.spark.sql.hive.client.HiveClientImpl - Warehouse location for Hive client (version 1.2.1) is file:/E:/release/spark-warehouse
18:02:41.396 INFO  [main] org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef - Registered StateStoreCoordinator endpoint
18:02:41.407 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ods_release.ods_01_release_session
18:02:41.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
18:02:41.847 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
18:02:41.861 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:02:41.862 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:02:42.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:02:42.038 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:02:42.080 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.098 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.099 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.099 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.099 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.100 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:42.101 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:02:42.119 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: array<string>
18:02:42.720 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_session
18:02:42.742 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: release_status
18:02:42.743 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_num
18:02:42.744 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: device_type
18:02:42.745 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: sources
18:02:42.745 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: channels
18:02:42.746 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. idcard') as idcard
18:02:42.810 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: (cast(date_format(now(),' yyyy') as int) - cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{6})(\\d{4})',2) as int)) as avg
18:02:42.903 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: cast(regexp_extract(get_json_object(exts,'$.idcard'),'(\\d{16})(\\d{1})()',2) as int) % 2 as gender
18:02:42.907 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. area_code') as area_code
18:02:42.908 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. Longitude') as longitude
18:02:42.910 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. latitude') as latitude
18:02:42.911 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. matter_id') as matter_id
18:02:42.912 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. model_code') as model_code
18:02:42.913 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. model_version') as model_version
18:02:42.915 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: get_json_object(exts,'$. aid') as aid
18:02:42.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: ct
18:02:42.916 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: bdp_day
18:02:42.921 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:42.925 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:42.938 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:42.939 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:42.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:42.950 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:42.959 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:42.960 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:42.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:42.970 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:42.988 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:42.988 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:42.999 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:42.999 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:43.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:43.009 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:43.018 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:43.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:43.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:43.027 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:43.036 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:43.037 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:43.048 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:43.049 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:43.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:43.058 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:43.067 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: default
18:02:43.067 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: default	
18:02:43.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
18:02:43.748 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
18:02:43.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:02:43.754 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:02:43.790 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:02:43.791 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:02:43.825 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.825 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.826 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.828 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:02:43.828 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:02:43.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
18:02:43.867 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
18:02:43.879 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
18:02:43.880 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
18:02:44.231 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
18:02:44.235 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
18:02:44.239 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
18:02:44.261 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
18:02:44.271 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
18:02:45.168 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 535.4602 ms
18:02:45.696 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 76.7816 ms
18:02:45.881 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 303.2 KB, free 1993.8 MB)
18:02:46.101 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.8 KB, free 1993.8 MB)
18:02:46.104 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.182.1:6047 (size: 25.8 KB, free: 1994.1 MB)
18:02:46.110 INFO  [main] org.apache.spark.SparkContext - Created broadcast 0 from show at DWReleaseCustomer.scala:58
18:02:46.120 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
18:02:46.818 INFO  [main] org.apache.spark.SparkContext - Starting job: show at DWReleaseCustomer.scala:58
18:02:46.838 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 4 (show at DWReleaseCustomer.scala:58)
18:02:46.841 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 0 (show at DWReleaseCustomer.scala:58) with 1 output partitions
18:02:46.842 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (show at DWReleaseCustomer.scala:58)
18:02:46.843 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
18:02:46.845 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
18:02:46.850 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:58), which has no missing parents
18:02:47.970 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 23.7 KB, free 1993.8 MB)
18:02:47.976 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1993.7 MB)
18:02:47.976 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.182.1:6047 (size: 10.4 KB, free: 1994.1 MB)
18:02:47.978 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1004
18:02:47.998 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[4] at show at DWReleaseCustomer.scala:58) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18:02:47.999 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 4 tasks
18:02:48.092 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 0
18:02:48.113 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5373 bytes)
18:02:48.117 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5373 bytes)
18:02:48.122 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 5373 bytes)
18:02:48.123 INFO  [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 5373 bytes)
18:02:48.140 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
18:02:48.140 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
18:02:48.141 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
18:02:48.140 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
18:02:48.523 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 55.4939 ms
18:02:48.603 INFO  [Executor task launch worker for task 3] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://Host01:9000/spark/release/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
18:02:48.603 INFO  [Executor task launch worker for task 1] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://Host01:9000/spark/release/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
18:02:48.604 INFO  [Executor task launch worker for task 2] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://Host01:9000/spark/release/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
18:02:48.604 INFO  [Executor task launch worker for task 0] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://Host01:9000/spark/release/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
18:02:49.813 INFO  [Executor task launch worker for task 3] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:02:49.814 INFO  [Executor task launch worker for task 2] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:02:49.815 INFO  [Executor task launch worker for task 0] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:02:49.890 INFO  [Executor task launch worker for task 1] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:02:50.300 INFO  [Executor task launch worker for task 0] org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1654 bytes result sent to driver
18:02:50.300 INFO  [Executor task launch worker for task 3] org.apache.spark.executor.Executor - Finished task 3.0 in stage 0.0 (TID 3). 1654 bytes result sent to driver
18:02:50.300 INFO  [Executor task launch worker for task 2] org.apache.spark.executor.Executor - Finished task 2.0 in stage 0.0 (TID 2). 1654 bytes result sent to driver
18:02:50.315 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 2221 ms on localhost (executor driver) (1/4)
18:02:50.319 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 0.0 (TID 3) in 2197 ms on localhost (executor driver) (2/4)
18:02:50.325 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 0.0 (TID 2) in 2208 ms on localhost (executor driver) (3/4)
18:04:20.804 INFO  [Executor task launch worker for task 1] org.apache.spark.executor.Executor - Finished task 1.0 in stage 0.0 (TID 1). 1826 bytes result sent to driver
18:04:22.037 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 0.0 (TID 1) in 93901 ms on localhost (executor driver) (4/4)
18:04:22.442 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
18:04:23.043 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (show at DWReleaseCustomer.scala:58) finished in 94.534 s
18:04:23.359 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
18:04:23.511 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
18:04:23.551 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 1)
18:04:23.619 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
18:04:24.442 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:58), which has no missing parents
18:04:25.791 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.7 KB, free 1993.7 MB)
18:04:25.917 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1993.7 MB)
18:04:26.034 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.182.1:6047 (size: 2.2 KB, free: 1994.1 MB)
18:04:26.213 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1004
18:04:26.708 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at DWReleaseCustomer.scala:58) (first 15 tasks are for partitions Vector(0))
18:04:26.720 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
18:04:26.949 INFO  [dispatcher-event-loop-3] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)
18:04:26.963 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 4)
18:04:29.168 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:04:29.439 INFO  [Executor task launch worker for task 4] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 652 ms
18:04:31.104 INFO  [Executor task launch worker for task 4] org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 4). 2243 bytes result sent to driver
18:04:31.128 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 4) in 4313 ms on localhost (executor driver) (1/1)
18:04:31.129 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
18:04:31.129 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (show at DWReleaseCustomer.scala:58) finished in 4.314 s
18:04:31.545 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 0 finished: show at DWReleaseCustomer.scala:58, took 104.710855 s
18:04:35.792 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: dwd_release.dw_release_customer
18:04:37.177 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
18:04:37.235 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
18:04:40.872 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:40.968 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.013 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.014 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.015 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.016 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.017 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:04:41.017 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.018 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.019 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.020 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.021 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:04:41.021 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:04:54.049 INFO  [main] org.apache.hadoop.hive.common.FileUtils - Creating directory if it doesn't exist: hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1
18:05:13.807 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: ods_release
18:05:13.829 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: ods_release	
18:05:14.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:05:14.742 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:05:14.864 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=ods_release tbl=ods_01_release_session
18:05:14.865 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=ods_release tbl=ods_01_release_session	
18:05:14.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.901 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.902 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.903 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:05:14.903 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:05:15.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_config_value: name=hive.metastore.try.direct.sql defaultValue=true
18:05:15.247 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_config_value: name=hive.metastore.try.direct.sql defaultValue=true	
18:05:15.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partitions_by_filter : db=ods_release tbl=ods_01_release_session
18:05:15.300 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_partitions_by_filter : db=ods_release tbl=ods_01_release_session	
18:05:16.645 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: isnotnull(bdp_day#9),(bdp_day#9 = 2019-09-24)
18:05:16.647 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: isnotnull(release_status#2),(release_status#2 = 01)
18:05:16.679 INFO  [main] org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<release_session: string, release_status: string, device_num: string, device_type: string, sources: string ... 6 more fields>
18:05:16.736 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Pushed Filters: IsNotNull(release_status),EqualTo(release_status,01)
18:05:16.777 INFO  [main] org.apache.spark.sql.execution.datasources.PrunedInMemoryFileIndex - Selected 1 partitions out of 1, pruned 0.0% partitions.
18:05:18.177 INFO  [main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:05:18.254 INFO  [main] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:05:23.434 INFO  [main] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 3559.2494 ms
18:05:24.300 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 303.2 KB, free 1993.4 MB)
18:05:24.804 INFO  [main] org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.8 KB, free 1993.4 MB)
18:05:24.805 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.182.1:6047 (size: 25.8 KB, free: 1994.0 MB)
18:05:24.858 INFO  [main] org.apache.spark.SparkContext - Created broadcast 3 from insertInto at SparkHelper.scala:36
18:05:24.880 INFO  [main] org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 11968796 bytes, open cost is considered as scanning 4194304 bytes.
18:05:28.731 INFO  [main] org.apache.spark.SparkContext - Starting job: insertInto at SparkHelper.scala:36
18:05:28.798 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Registering RDD 11 (insertInto at SparkHelper.scala:36)
18:05:28.798 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Got job 1 (insertInto at SparkHelper.scala:36) with 4 output partitions
18:05:28.798 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (insertInto at SparkHelper.scala:36)
18:05:28.798 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
18:05:28.798 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
18:05:28.814 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:36), which has no missing parents
18:05:29.218 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 23.6 KB, free 1993.4 MB)
18:05:29.240 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.3 KB, free 1993.4 MB)
18:05:29.241 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.182.1:6047 (size: 10.3 KB, free: 1994.0 MB)
18:05:29.294 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1004
18:05:29.316 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18:05:29.316 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 4 tasks
18:05:29.385 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 5, localhost, executor driver, partition 0, ANY, 5373 bytes)
18:05:29.386 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 6, localhost, executor driver, partition 1, ANY, 5373 bytes)
18:05:29.409 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 7, localhost, executor driver, partition 2, ANY, 5373 bytes)
18:05:29.411 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 8, localhost, executor driver, partition 3, ANY, 5373 bytes)
18:05:29.412 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 5)
18:05:29.622 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 7)
18:05:29.728 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 8)
18:05:29.728 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 6)
18:05:31.752 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 1341.973 ms
18:05:31.945 INFO  [Executor task launch worker for task 5] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://Host01:9000/spark/release/bdp_day=2019-09-24/15603638400003h4gka4h, range: 0-11968796, partition values: [2019-09-24]
18:05:31.945 INFO  [Executor task launch worker for task 8] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://Host01:9000/spark/release/bdp_day=2019-09-24/15603638400003h4gka4h, range: 35906388-43680880, partition values: [2019-09-24]
18:05:31.946 INFO  [Executor task launch worker for task 7] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://Host01:9000/spark/release/bdp_day=2019-09-24/15603638400003h4gka4h, range: 23937592-35906388, partition values: [2019-09-24]
18:05:31.946 INFO  [Executor task launch worker for task 6] org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: hdfs://Host01:9000/spark/release/bdp_day=2019-09-24/15603638400003h4gka4h, range: 11968796-23937592, partition values: [2019-09-24]
18:05:34.179 INFO  [Executor task launch worker for task 7] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:05:34.179 INFO  [Executor task launch worker for task 5] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:05:34.611 INFO  [Executor task launch worker for task 7] org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 7). 1568 bytes result sent to driver
18:05:34.612 INFO  [Executor task launch worker for task 5] org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 5). 1525 bytes result sent to driver
18:05:34.655 INFO  [Executor task launch worker for task 8] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:05:34.665 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 7) in 5279 ms on localhost (executor driver) (1/4)
18:05:34.666 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 5) in 5350 ms on localhost (executor driver) (2/4)
18:05:34.737 INFO  [Executor task launch worker for task 8] org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 8). 1611 bytes result sent to driver
18:05:34.738 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 8) in 5328 ms on localhost (executor driver) (3/4)
18:05:34.766 INFO  [Executor task launch worker for task 6] org.apache.parquet.filter2.compat.FilterCompat - Filtering using predicate: and(noteq(release_status, null), eq(release_status, Binary{"01"}))
18:06:24.251 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 1
18:06:24.904 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 81
18:06:24.904 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 6
18:06:24.904 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 4
18:06:27.095 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned shuffle 0
18:06:29.699 INFO  [dispatcher-event-loop-0] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.182.1:6047 in memory (size: 10.4 KB, free: 1994.0 MB)
18:06:29.803 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 7
18:06:29.803 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 8
18:06:29.803 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 2
18:06:29.804 INFO  [dispatcher-event-loop-3] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on 192.168.182.1:6047 in memory (size: 25.8 KB, free: 1994.1 MB)
18:06:29.806 INFO  [dispatcher-event-loop-1] org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on 192.168.182.1:6047 in memory (size: 2.2 KB, free: 1994.1 MB)
18:06:29.806 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 5
18:06:29.806 INFO  [Spark Context Cleaner] org.apache.spark.ContextCleaner - Cleaned accumulator 3
18:06:31.547 INFO  [Executor task launch worker for task 6] org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 6). 1783 bytes result sent to driver
18:06:31.786 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 6) in 62401 ms on localhost (executor driver) (4/4)
18:06:31.786 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
18:06:31.887 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (insertInto at SparkHelper.scala:36) finished in 62.571 s
18:06:31.887 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
18:06:31.912 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - running: Set()
18:06:31.912 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
18:06:31.912 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - failed: Set()
18:06:32.028 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:36), which has no missing parents
18:06:32.388 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 177.6 KB, free 1993.6 MB)
18:06:32.419 INFO  [dag-scheduler-event-loop] org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 65.2 KB, free 1993.5 MB)
18:06:32.443 INFO  [dispatcher-event-loop-2] org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.182.1:6047 (size: 65.2 KB, free: 1994.0 MB)
18:06:32.444 INFO  [dag-scheduler-event-loop] org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1004
18:06:32.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at insertInto at SparkHelper.scala:36) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
18:06:32.444 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 4 tasks
18:06:32.453 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 9, localhost, executor driver, partition 0, ANY, 4726 bytes)
18:06:32.453 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 10, localhost, executor driver, partition 1, ANY, 4726 bytes)
18:06:32.453 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 3.0 (TID 11, localhost, executor driver, partition 2, ANY, 4726 bytes)
18:06:32.454 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 3.0 (TID 12, localhost, executor driver, partition 3, ANY, 4726 bytes)
18:06:32.454 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 9)
18:06:32.473 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Running task 2.0 in stage 3.0 (TID 11)
18:06:32.482 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 10)
18:06:32.503 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Running task 3.0 in stage 3.0 (TID 12)
18:06:32.822 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:06:32.822 INFO  [Executor task launch worker for task 12] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
18:06:32.833 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:06:32.833 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:06:32.833 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 non-empty blocks out of 4 blocks
18:06:32.833 INFO  [Executor task launch worker for task 10] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
18:06:32.833 INFO  [Executor task launch worker for task 11] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
18:06:32.833 INFO  [Executor task launch worker for task 9] org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
18:06:33.624 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 277.4341 ms
18:06:33.887 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 100.3245 ms
18:06:34.568 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:06:34.568 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:06:34.568 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:06:34.568 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 1
18:06:34.586 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:06:34.586 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:06:34.587 INFO  [Executor task launch worker for task 9] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:06:34.587 INFO  [Executor task launch worker for task 10] org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
18:06:34.736 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 124.1036 ms
18:06:35.327 INFO  [Executor task launch worker for task 12] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 133.2493 ms
18:06:35.366 INFO  [Executor task launch worker for task 11] org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 34.6733 ms
18:06:36.295 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@dc08757
18:06:36.297 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@2ca95c8d
18:06:36.297 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@706898f0
18:06:36.297 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat - creating new record writer...org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat@7dc7ff5f
18:06:36.426 INFO  [Executor task launch worker for task 10] org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
18:06:36.466 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
18:06:36.466 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
18:06:36.466 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
18:06:36.467 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - initialize serde with table properties.
18:06:36.467 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/_temporary/0/_temporary/attempt_20190925180634_0003_m_000003_0/bdp_day=2019-09-24/part-00003-99867fec-1997-4099-beec-bdf89302336a.c000
18:06:36.467 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/_temporary/0/_temporary/attempt_20190925180634_0003_m_000001_0/bdp_day=2019-09-24/part-00001-99867fec-1997-4099-beec-bdf89302336a.c000
18:06:36.467 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/_temporary/0/_temporary/attempt_20190925180634_0003_m_000000_0/bdp_day=2019-09-24/part-00000-99867fec-1997-4099-beec-bdf89302336a.c000
18:06:36.467 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - creating real writer to write at hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/_temporary/0/_temporary/attempt_20190925180634_0003_m_000002_0/bdp_day=2019-09-24/part-00002-99867fec-1997-4099-beec-bdf89302336a.c000
18:06:36.499 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression set to false
18:06:36.499 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression set to false
18:06:36.499 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression set to false
18:06:36.499 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression set to false
18:06:36.500 INFO  [Executor task launch worker for task 9] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
18:06:36.500 INFO  [Executor task launch worker for task 10] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
18:06:36.500 INFO  [Executor task launch worker for task 11] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
18:06:36.500 INFO  [Executor task launch worker for task 12] parquet.hadoop.codec.CodecConfig - Compression: UNCOMPRESSED
18:06:36.511 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
18:06:36.511 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
18:06:36.511 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
18:06:36.511 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet block size to 134217728
18:06:36.512 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
18:06:36.512 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
18:06:36.512 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
18:06:36.512 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet page size to 1048576
18:06:36.512 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
18:06:36.512 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
18:06:36.512 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
18:06:36.512 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Parquet dictionary page size to 1048576
18:06:36.512 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Dictionary is on
18:06:36.512 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Dictionary is on
18:06:36.513 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Dictionary is on
18:06:36.513 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Dictionary is on
18:06:36.513 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Validation is off
18:06:36.513 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Validation is off
18:06:36.513 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Validation is off
18:06:36.513 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Validation is off
18:06:36.514 INFO  [Executor task launch worker for task 12] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
18:06:36.514 INFO  [Executor task launch worker for task 11] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
18:06:36.514 INFO  [Executor task launch worker for task 9] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
18:06:36.515 INFO  [Executor task launch worker for task 10] parquet.hadoop.ParquetOutputFormat - Writer version is: PARQUET_1_0
18:06:37.898 INFO  [Executor task launch worker for task 11] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@2fa5c3b
18:06:37.899 INFO  [Executor task launch worker for task 12] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@54cdf312
18:06:37.900 INFO  [Executor task launch worker for task 9] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@66a127eb
18:06:37.902 INFO  [Executor task launch worker for task 10] org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper - real writer: parquet.hadoop.ParquetRecordWriter@31403ce
18:06:38.940 INFO  [Executor task launch worker for task 12] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,561,160
18:06:38.940 INFO  [Executor task launch worker for task 11] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,560,929
18:06:38.959 INFO  [Executor task launch worker for task 10] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,560,811
18:06:38.967 INFO  [Executor task launch worker for task 9] parquet.hadoop.InternalParquetRecordWriter - Flushing mem columnStore to file. allocated memory: 2,560,991
18:06:39.619 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 579,154B for [release_session] BINARY: 21,447 values, 579,077B raw, 579,077B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.650 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.650 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.650 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 579,127B for [release_session] BINARY: 21,446 values, 579,050B raw, 579,050B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.659 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
18:06:39.659 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
18:06:39.659 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
18:06:39.659 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [release_status] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
18:06:39.661 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.661 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
18:06:39.662 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
18:06:39.662 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
18:06:39.664 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.664 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.664 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [age] INT32: 21,446 values, 8B raw, 8B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.664 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 214,521B for [device_num] BINARY: 21,447 values, 214,478B raw, 214,478B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.669 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,447 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
18:06:39.669 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
18:06:39.670 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,447 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
18:06:39.677 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 2,785B for [gender] BINARY: 21,446 values, 2,754B raw, 2,754B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
18:06:39.677 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 5,446B for [device_type] BINARY: 21,446 values, 5,415B raw, 5,415B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
18:06:39.677 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 214,511B for [device_num] BINARY: 21,446 values, 214,468B raw, 214,468B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.678 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 3,235B for [area_code] BINARY: 21,446 values, 3,194B raw, 3,194B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
18:06:39.678 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
18:06:39.678 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [longitude] BINARY: 21,446 values, 8B raw, 8B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.678 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 471,909B for [idcard] BINARY: 21,447 values, 471,842B raw, 471,842B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.678 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 5,445B for [device_type] BINARY: 21,446 values, 5,414B raw, 5,414B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 3 entries, 15B raw, 3B comp}
18:06:39.679 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [age] INT32: 21,447 values, 8B raw, 8B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.679 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
18:06:39.679 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 2,790B for [gender] BINARY: 21,447 values, 2,759B raw, 2,759B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
18:06:39.679 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,137B for [sources] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 7 entries, 73B raw, 7B comp}
18:06:39.679 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
18:06:39.679 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 3,215B for [area_code] BINARY: 21,447 values, 3,174B raw, 3,174B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
18:06:39.680 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 43B for [channels] BINARY: 21,446 values, 12B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1 entries, 6B raw, 1B comp}
18:06:39.680 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
18:06:39.680 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [longitude] BINARY: 21,447 values, 8B raw, 8B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.680 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.682 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [age] INT32: 21,446 values, 8B raw, 8B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.682 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
18:06:39.682 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,447 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
18:06:39.683 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
18:06:39.683 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
18:06:39.683 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 471,887B for [idcard] BINARY: 21,446 values, 471,820B raw, 471,820B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.683 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 2,787B for [gender] BINARY: 21,446 values, 2,756B raw, 2,756B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
18:06:39.683 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [age] INT32: 21,446 values, 8B raw, 8B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.684 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
18:06:39.684 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
18:06:39.684 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 3,216B for [area_code] BINARY: 21,446 values, 3,175B raw, 3,175B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
18:06:39.684 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 2,791B for [gender] BINARY: 21,446 values, 2,760B raw, 2,760B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 10B raw, 2B comp}
18:06:39.684 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,447 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
18:06:39.684 INFO  [Executor task launch worker for task 10] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
18:06:39.684 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [longitude] BINARY: 21,446 values, 8B raw, 8B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.684 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 3,253B for [area_code] BINARY: 21,446 values, 3,212B raw, 3,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 2 entries, 20B raw, 2B comp}
18:06:39.684 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,447 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
18:06:39.685 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 33B for [longitude] BINARY: 21,446 values, 8B raw, 8B comp, 1 pages, encodings: [PLAIN, BIT_PACKED, RLE]
18:06:39.685 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
18:06:39.685 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 24,238B for [latitude] BINARY: 21,446 values, 24,181B raw, 24,181B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 339 entries, 5,248B raw, 339B comp}
18:06:39.685 INFO  [Executor task launch worker for task 12] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,447 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
18:06:39.685 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
18:06:39.686 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [matter_id] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
18:06:39.686 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
18:06:39.686 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
18:06:39.686 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,813B for [model_code] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 71B raw, 10B comp}
18:06:39.686 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
18:06:39.687 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 10,811B for [model_version] BINARY: 21,446 values, 10,776B raw, 10,776B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 10 entries, 61B raw, 10B comp}
18:06:39.687 INFO  [Executor task launch worker for task 11] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
18:06:39.687 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 8,132B for [aid] BINARY: 21,446 values, 8,095B raw, 8,095B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 5 entries, 40B raw, 5B comp}
18:06:39.687 INFO  [Executor task launch worker for task 9] parquet.hadoop.ColumnChunkPageWriteStore - written 29,590B for [ct] INT64: 21,446 values, 29,543B raw, 29,543B comp, 1 pages, encodings: [BIT_PACKED, PLAIN_DICTIONARY, RLE], dic { 1,357 entries, 10,856B raw, 1,357B comp}
18:06:44.853 INFO  [Executor task launch worker for task 9] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925180634_0003_m_000000_0' to hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/_temporary/0/task_20190925180634_0003_m_000000
18:06:44.901 INFO  [Executor task launch worker for task 9] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925180634_0003_m_000000_0: Committed
18:06:45.030 INFO  [Executor task launch worker for task 9] org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 9). 2660 bytes result sent to driver
18:06:45.076 INFO  [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 9) in 12624 ms on localhost (executor driver) (1/4)
18:06:45.595 INFO  [Executor task launch worker for task 12] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925180634_0003_m_000003_0' to hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/_temporary/0/task_20190925180634_0003_m_000003
18:06:45.595 INFO  [Executor task launch worker for task 12] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925180634_0003_m_000003_0: Committed
18:06:45.601 INFO  [Executor task launch worker for task 12] org.apache.spark.executor.Executor - Finished task 3.0 in stage 3.0 (TID 12). 2617 bytes result sent to driver
18:06:45.638 INFO  [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 3.0 (TID 12) in 13185 ms on localhost (executor driver) (2/4)
18:06:45.699 INFO  [Executor task launch worker for task 10] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925180634_0003_m_000001_0' to hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/_temporary/0/task_20190925180634_0003_m_000001
18:06:45.700 INFO  [Executor task launch worker for task 10] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925180634_0003_m_000001_0: Committed
18:06:45.701 INFO  [Executor task launch worker for task 10] org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 10). 2617 bytes result sent to driver
18:06:45.744 INFO  [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 10) in 13291 ms on localhost (executor driver) (3/4)
18:06:45.920 INFO  [Executor task launch worker for task 11] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_20190925180634_0003_m_000002_0' to hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/_temporary/0/task_20190925180634_0003_m_000002
18:06:45.985 INFO  [Executor task launch worker for task 11] org.apache.spark.mapred.SparkHadoopMapRedUtil - attempt_20190925180634_0003_m_000002_0: Committed
18:06:45.986 INFO  [Executor task launch worker for task 11] org.apache.spark.executor.Executor - Finished task 2.0 in stage 3.0 (TID 11). 2617 bytes result sent to driver
18:06:45.987 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 3.0 (TID 11) in 13534 ms on localhost (executor driver) (4/4)
18:06:45.987 INFO  [task-result-getter-0] org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
18:06:45.988 INFO  [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (insertInto at SparkHelper.scala:36) finished in 13.536 s
18:06:46.021 INFO  [main] org.apache.spark.scheduler.DAGScheduler - Job 1 finished: insertInto at SparkHelper.scala:36, took 77.240357 s
18:06:46.782 INFO  [main] org.apache.spark.sql.execution.datasources.FileFormatWriter - Job null committed.
18:06:47.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
18:06:47.019 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
18:06:47.674 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
18:06:47.675 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
18:06:47.827 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.880 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.880 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.880 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.881 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.882 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:06:47.883 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.884 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.885 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:47.886 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:06:47.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
18:06:47.916 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
18:06:48.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
18:06:48.045 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
18:06:48.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
18:06:48.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
18:06:48.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
18:06:48.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
18:06:48.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: partition_name_has_valid_characters
18:06:48.046 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
18:06:48.225 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
18:06:48.225 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
18:06:48.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dwd_release tbl=dw_release_customer[2019-09-24]
18:06:48.273 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dwd_release tbl=dw_release_customer[2019-09-24]	
18:06:48.371 INFO  [main] org.apache.hadoop.hive.common.FileUtils - deleting  hdfs://Host01:9000/spark/release1/bdp_day=2019-09-24/000000_0
18:06:48.425 INFO  [main] org.apache.hadoop.fs.TrashPolicyDefault - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
18:06:48.541 ERROR [main] org.apache.hadoop.hdfs.KeyProviderCache - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
18:06:48.618 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/bdp_day=2019-09-24/part-00000-99867fec-1997-4099-beec-bdf89302336a.c000, dest: hdfs://Host01:9000/spark/release1/bdp_day=2019-09-24/part-00000-99867fec-1997-4099-beec-bdf89302336a.c000, Status:true
18:06:48.679 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/bdp_day=2019-09-24/part-00001-99867fec-1997-4099-beec-bdf89302336a.c000, dest: hdfs://Host01:9000/spark/release1/bdp_day=2019-09-24/part-00001-99867fec-1997-4099-beec-bdf89302336a.c000, Status:true
18:06:48.691 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/bdp_day=2019-09-24/part-00002-99867fec-1997-4099-beec-bdf89302336a.c000, dest: hdfs://Host01:9000/spark/release1/bdp_day=2019-09-24/part-00002-99867fec-1997-4099-beec-bdf89302336a.c000, Status:true
18:06:48.704 INFO  [main] hive.ql.metadata.Hive - Replacing src:hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/bdp_day=2019-09-24/part-00003-99867fec-1997-4099-beec-bdf89302336a.c000, dest: hdfs://Host01:9000/spark/release1/bdp_day=2019-09-24/part-00003-99867fec-1997-4099-beec-bdf89302336a.c000, Status:true
18:06:48.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_partition_with_auth : db=dwd_release tbl=dw_release_customer[2019-09-24]
18:06:48.710 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=dwd_release tbl=dw_release_customer[2019-09-24]	
18:06:48.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: alter_partition : db=dwd_release tbl=dw_release_customer
18:06:48.768 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=alter_partition : db=dwd_release tbl=dw_release_customer	
18:06:48.769 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - New partition values:[2019-09-24]
18:06:48.874 WARN  [main] hive.log - Updating partition stats fast for: dw_release_customer
18:06:48.878 WARN  [main] hive.log - Updated size to 5591197
18:06:49.518 INFO  [main] hive.ql.metadata.Hive - New loading path = hdfs://Host01:9000/spark/release1/.hive-staging_hive_2019-09-25_18-04-53_909_2975071671749007138-1/-ext-10000/bdp_day=2019-09-24 with partSpec {bdp_day=2019-09-24}
18:06:50.074 INFO  [main] org.apache.spark.sql.execution.SparkSqlParser - Parsing command: `dwd_release`.`dw_release_customer`
18:06:50.166 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_database: dwd_release
18:06:50.167 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_database: dwd_release	
18:06:50.205 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
18:06:50.206 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
18:06:50.278 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore - 0: get_table : db=dwd_release tbl=dw_release_customer
18:06:50.278 INFO  [main] org.apache.hadoop.hive.metastore.HiveMetaStore.audit - ugi=²ܲ	ip=unknown-ip-addr	cmd=get_table : db=dwd_release tbl=dw_release_customer	
18:06:50.723 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.724 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.724 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.724 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.724 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.725 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.726 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.726 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: int
18:06:50.726 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.726 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.727 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.727 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.727 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.727 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.728 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.728 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: string
18:06:50.762 INFO  [main] org.apache.spark.sql.catalyst.parser.CatalystSqlParser - Parsing command: bigint
18:06:51.162 INFO  [Thread-1] org.apache.spark.SparkContext - Invoking stop() from shutdown hook
18:06:51.415 INFO  [Thread-1] org.spark_project.jetty.server.AbstractConnector - Stopped Spark@792f2a8f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
18:06:51.499 INFO  [Thread-1] org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.182.1:4040
18:06:55.339 INFO  [dispatcher-event-loop-1] org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
18:06:56.858 INFO  [Thread-1] org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
18:06:56.900 INFO  [Thread-1] org.apache.spark.storage.BlockManager - BlockManager stopped
18:06:56.919 INFO  [Thread-1] org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
18:06:57.067 INFO  [dispatcher-event-loop-0] org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
18:06:57.230 INFO  [Thread-1] org.apache.spark.SparkContext - Successfully stopped SparkContext
18:06:57.267 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Shutdown hook called
18:06:57.352 INFO  [Thread-1] org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\曹曹\AppData\Local\Temp\spark-5f89d779-0f8c-4f60-8795-d2b24963f1db
